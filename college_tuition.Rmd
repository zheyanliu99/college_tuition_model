---
title: "Assignment 2"
author: "Zheyan Liu"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
--- 


```{r, include=FALSE}
library(RNHANES)
library(tidyverse)
library(summarytools)
library(leaps)
library(ISLR)
library(glmnet)
library(caret)
library(corrplot)
library(plotmo)
library(pls)
library(ggplot2)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "95%"
)

theme_set(theme_minimal() + theme(legend.position = 'bottom'))

options(
  ggplot2.continuous.colour = 'viridis',
  ggplot2.continuous.fill = 'viridis'
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

\newpage

## Intro and data preparation

we build nonlinear models using the “College” data. The dataset
contains statistics for 565 US Colleges from a previous issue of US News and World Report.
The response variable is the out-of-state tuition (Outstate).

### Read data

Drop college column

```{r}
df = 
  read_csv('data/College.csv', show_col_types = FALSE) %>% 
  janitor::clean_names() %>% 
  select(-college)
```

### Split the dataset into training and testing

Partition the dataset into two parts: training data (80%) and test data (20%).

```{r}
trainRows <- createDataPartition(y = df$outstate, p = 0.8, list = FALSE)
training_df = df[trainRows, ]
testing_df = df[-trainRows, ]

x_train <- model.matrix(outstate~.,training_df)[,-1]
y_train <- training_df$outstate

x_test <- model.matrix(outstate~.,testing_df)[,-1]
y_test <- testing_df$outstate
```


## (a) Perform exploratory data analysis using the training data

There are `r ncol(training_df)` variables in the data and `r nrow(training_df)` observations.

### summary statistics

All variables are continuous

```{r}
summary(training_df)
```
### histogram of response variable

Distribution of outstate is close to normal distribution, much outstate is around 10000 except a second peak around 17500

```{r}
ggplot(training_df, aes(x=outstate)) + 
  geom_histogram(bins = 32)
```

### correlation of response vs. predictors

Correlation plot shows that some variables are highly correlated with outstate and there is multicollinearity.

```{r}
corrplot::corrplot(cor(training_df %>% select(where(is.numeric)) %>% relocate(outstate)), method = "circle", type = "full")
```
Plot scatterplot of response variables with selection of highly correlated predictors including top10perc, room_board, ph_d, terminal, s_f_ratio and expend. Only s_f_ratio is negatively correlated.

```{r}
library(patchwork)
p1 = ggplot(training_df, aes(x=top10perc, y=outstate)) + geom_point(alpha=0.3)
p2 = ggplot(training_df, aes(x=room_board, y=outstate)) + geom_point(alpha=0.3)
p3 = ggplot(training_df, aes(x=ph_d, y=outstate)) + geom_point(alpha=0.3)
p4 = ggplot(training_df, aes(x=terminal, y=outstate)) + geom_point(alpha=0.3)
p5 = ggplot(training_df, aes(x=s_f_ratio, y=outstate)) + geom_point(alpha=0.3)
p6 = ggplot(training_df, aes(x=expend, y=outstate)) + geom_point(alpha=0.3)

(p1 + p2 + p3)/(p4 + p5 + p6)
```


## (b) Fit smoothing spline models using Terminal as the only predictor of Outstate

### Fit on train data

```{r}
set.seed(777)
fit.ss <- smooth.spline(training_df$terminal, training_df$outstate)

terminal.grid <- seq(from = 20, to = 110, by = 1)

pred.ss <- predict(fit.ss,
                   x = terminal.grid)

pred.ss.df <- data.frame(pred = pred.ss$y,
                         x = terminal.grid)

p <- ggplot(data = training_df, aes(x = terminal, y = outstate)) +
     geom_point(color = rgb(.2, .4, .2, .5))

p +
geom_line(aes(x = terminal.grid, y = pred), data = pred.ss.df,
          color = rgb(.8, .1, .1, 1)) + theme_bw()
```
Degree of freedom is `r fit.ss$df`

### Fit on test data 

```{r}
set.seed(777)
fit.ss <- smooth.spline(training_df$terminal, training_df$outstate)

terminal.grid <- seq(from = 20, to = 110, by = 1)

pred.ss <- predict(fit.ss,
                   x = terminal.grid)

pred.ss.df <- data.frame(pred = pred.ss$y,
                         x = terminal.grid)

p <- ggplot(data = testing_df, aes(x = terminal, y = outstate)) +
     geom_point(color = rgb(.2, .4, .2, .5))

p +
geom_line(aes(x = terminal.grid, y = pred), data = pred.ss.df,
          color = rgb(.8, .1, .1, 1)) + theme_bw()
```
Model captures the trend of outstate on the test set as well.

### Generalized cross-validation and visualize it

Set df candidates to a sequences from 2 to 20 with step as 2, fit and record the result

```{r}
pred.ss.df <- data.frame(pred = pred.ss$y,
                         x = terminal.grid)

flag = TRUE

for (df in seq(2, 20, by=2)){
  fit.ss <- smooth.spline(training_df$terminal, training_df$outstate, df = df)
  pred.ss <- predict(fit.ss,
                   x = terminal.grid)
  pred.ss.df <- data.frame(pred = pred.ss$y,
                           x = terminal.grid,
                           df = df)
  if (flag){
    pred.ss.df.all = pred.ss.df
    flag = FALSE
  }
  else{
    pred.ss.df.all = rbind(pred.ss.df.all, pred.ss.df)
  }
}
```

Visualize it. The larger the df, the more non-linear fit.

```{r}
p +
geom_line(aes(x = x, y = pred, group = df, color = df), data = pred.ss.df.all)
```


## (c) Fit a generalized additive model (GAM) using all the predictors. 

### Training

```{r}
library(mgcv)
outcome = "outstate"
variables = colnames(x_train)

# fully parameterized
f = as.formula(
  paste(outcome, 
        paste(variables, collapse = " + "), 
        sep = " ~ "))

gam.m1 <- gam(f, data = training_df)
gam.m2 <- gam(outstate ~ apps + accept + enroll + top10perc + top25perc + f_undergrad + 
    p_undergrad + room_board + books + personal + ph_d + s(terminal) + 
    s_f_ratio + perc_alumni + expend + grad_rate, data = training_df)
gam.m3 <- gam(outstate ~ apps + accept + enroll + top10perc + top25perc + f_undergrad + 
    p_undergrad + room_board + books + personal + ph_d + s(terminal) + 
    te(s_f_ratio, perc_alumni) + expend + grad_rate, data = training_df)

anova(gam.m1, gam.m2, gam.m3, test = "F")
```
Fails to reject they have the same deviance.

Model result for the best GAM model: model3

```{r}
summary(gam.m3)
```

### Visualization

```{r}
plot(gam.m2)
```

```{r}
vis.gam(gam.m3, view = c("s_f_ratio","perc_alumni"),
color = "topo")
```
### Prediction on the test set

```{r}
pred_y = predict.gam(gam.m3, newdata = as.tibble(x_test))
actual_y = y_test

# Metrics
rmse = sqrt(mean((pred_y - actual_y)^2)) # Calculate test MSE
mae = mean(abs(pred_y - actual_y))
```

Final RMSE is `r rmse`, MAE is `r mae`, while the median in the test set is `r median(actual_y)`. The predicton error is acceptable.

We can also see from the prediction compare plot that the model fits well.

```{r}
y_compare_df = 
  tibble(pred = pred_y,
         actual = actual_y) %>% 
  arrange(actual) %>% 
    pivot_longer(
      cols = pred:actual,
      names_to = 'type',
      values_to = 'outstate'
    ) 

y_compare_df %>% 
  mutate(
    num = seq(1, nrow(y_compare_df)),
    type = as.factor(type)
         ) %>% 
  ggplot(aes(x = num, y = outstate), color = type) + 
  geom_point(aes(colour = type)) + 
  theme(legend.position="right")

```


## (d) Train a multivariate adaptive regression spline (MARS) model using all the predictors

### Training MARS

```{r}
mars_grid <- expand.grid(degree = 1:3, 
                         nprune = 2:15)
ctrl1 <- trainControl(method = "cv", number = 10)
mars.fit <- train(x_train, y_train,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

ggplot(mars.fit)

# parameters for optimized result
mars.fit$bestTune

# Final model
summary(mars.fit$finalModel)
```
### partial dependence

partial dependence between grad_rate and enroll

```{r}
p1 <- pdp::partial(mars.fit, pred.var = c("terminal"), grid.resolution = 10) %>% autoplot()

p2 <- pdp::partial(mars.fit, pred.var = c("grad_rate", "enroll"), 
                   grid.resolution = 10) %>%
      pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
                       screen = list(z = 20, x = -60))
library(gridExtra)
# x_train
grid.arrange(p1, p2, ncol = 2)
```
### Prediction on the test set

```{r}
pred_y = predict(mars.fit, newdata = as.tibble(x_test))
actual_y = y_test

# Metrics
rmse = sqrt(mean((pred_y - actual_y)^2)) # Calculate test MSE
mae = mean(abs(pred_y - actual_y))
```

Final RMSE is `r rmse`, MAE is `r mae`, while the median in the test set is `r median(actual_y)`. The predicton error is acceptable.

We can also see from the prediction compare plot that the model fits well.

```{r}
y_compare_df = 
  tibble(pred = pred_y,
         actual = actual_y) %>% 
  arrange(actual) %>% 
    pivot_longer(
      cols = pred:actual,
      names_to = 'type',
      values_to = 'outstate'
    ) 

y_compare_df %>% 
  mutate(
    num = seq(1, nrow(y_compare_df)),
    type = as.factor(type)
         ) %>% 
  ggplot(aes(x = num, y = outstate), color = type) + 
  geom_point(aes(colour = type)) + 
  theme(legend.position="right")

```

## (e) Model selection











